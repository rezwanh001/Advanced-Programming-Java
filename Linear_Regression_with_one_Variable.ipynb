{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Regression with one Variable.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezwanh001/Advanced-Programming-Java/blob/master/Linear_Regression_with_one_Variable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "UF_IXmxNypvo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Check the GPU "
      ]
    },
    {
      "metadata": {
        "id": "pK1QYhu5yjd3",
        "colab_type": "code",
        "outputId": "ed9f7dc8-7f7e-4840-fef0-b20c45f429f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q8_jlKPD-cIu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Mount the gdrive"
      ]
    },
    {
      "metadata": {
        "id": "cFdV7Jsz-bVO",
        "colab_type": "code",
        "outputId": "795c4f4b-4bd5-44a5-cf06-8c88280143c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BfEMWMWfhjbp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model and Cost Function:\n",
        "----------------------------------------------\n",
        "      \n",
        "  * `m` = Number of training examples.\n",
        "\n",
        "  *  `X's` = Input variable/Features.\n",
        "\n",
        "  * ` y's `= Outout variable/Target variable.\n",
        "\n",
        "  * `(X,y)` = one training example.\n",
        "\n",
        "  * ($X^{(i)}, y^{(i)}$) = $i$-th training example.\n",
        "  \n",
        "  \n",
        "  \n",
        "So here's how this supervised learning algorithm works. To describe the supervised learning problem slightly more formally, our goal is, given a training set, to learn a function `h : X → y` so that `h(x)` is a “good” predictor for the corresponding value of `y`. For historical reasons, this function `h` is called a hypothesis. Seen pictorially, the process is therefore like this:\n",
        "  \n",
        "   ![hypo](https://user-images.githubusercontent.com/15044221/52900513-ed1db480-3220-11e9-9b11-a0f96a5a7854.png)\n",
        "\n",
        "    Fig-1: The Process the supervised learning algorithms.\n",
        "   \n",
        " When the target variable that we’re trying to predict is continuous, such as in our housing example, we call the learning problem a regression problem. When `y` can take on only a small number of discrete values (such as if, given the living area, we wanted to predict if a dwelling is a house or an apartment, say), we call it a classification problem.\n",
        " \n",
        "   * ***How do we represent `h`?***\n",
        "        \n",
        "        $h_\\theta(X)$ = $\\theta_0 + \\theta_1(X)$        --------------------------------------------------------------------------(i)\n",
        "        \n",
        "  \n",
        "  \n",
        "  *  ***How to fit the best possible straight line in our data?***\n",
        "     \n",
        "    * Hypothesis: $h_\\theta(x)$ = $\\theta_0 + \\theta_1x$  -----------------------------------------------------------(ii)\n",
        "    \n",
        "    * $\\theta_{i}$'s  = Parameters.\n",
        "    \n",
        "    * Idea: Choose $\\theta_0$, $\\theta_1$ so that $h_\\theta(x)$ is close to $y$ for our training examples$(X, y)$\n",
        "    \n",
        "    * Let's formalize this. So linear regression, what we're going to do is, I'm going to want to solve a minimization problem. So I'll write minimize over $\\theta_0$, $\\theta_1$ with following formula:\n",
        "    \n",
        "         $\\underset{\\theta_0, \\theta_1}{minimize}$ =$\\frac{1}{2m}$ $\\sum_{i=1}^{m}$ $(h_\\theta(X^{(i)}) - y^{(i)})^2$      -------------------------------(iii)\n",
        "    \n",
        "    \n",
        "   * ***Cost function:***\n",
        "   \n",
        "      *  $J(\\theta_0, \\theta_1)$ = $\\frac{1}{2m}$ $\\sum_{i=1}^{m}$ $(h_\\theta(X^{(i)}) - y^{(i)})^2$      -------------------------------------------(iv)\n",
        "     \n",
        "     This cost function is also known as Squared Error Function.\n",
        "     \n",
        "      It turns out that why do we take the squares of the errors. It turns out that these squared error cost function is a reasonable choice and works well for problems for most regression programs. There are other cost functions that will work pretty well. But the square cost function is probably the most commonly used one for regression problems.\n",
        "      \n",
        "      We can measure the accuracy of our hypothesis function by using a **cost function**. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from $x$'s and the actual output $y$'s.\n",
        "      \n",
        "      * $J(\\theta_0, \\theta_1)$ =  $\\frac{1}{2m}$ $\\sum_{i=1}^{m}$ $(\\hat{y_i} - y_i)^2$  = $\\frac{1}{2m}$ $\\sum_{i=1}^{m}$ $(h_\\theta(X^{(i)}) - y^{(i)})^2$      ----------(v)\n",
        "      \n",
        "   To break it apart, it is $\\frac{1}{2}\\bar{x}$ where $\\bar{x}$ is the mean of the squares of $h_\\theta (x_{i}) - y_{i}$ or the difference between the predicted value and the actual value.\n",
        "      \n",
        "      This function is otherwise called the \"Squared error function\", or \"Mean squared error\". The mean is halved $\\left(\\frac{1}{2}\\right)$ as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the $\\frac{1}{2}$ term.\n",
        "\n",
        "    \n",
        "* ***What the cost function is doing, and why we want to use it?***\n",
        " \n",
        " * Hypothesis: \n",
        " \n",
        "    $h_\\theta(x)$ = $\\theta_0 + \\theta_1x$\n",
        " \n",
        " * Parameters:\n",
        " \n",
        "    $\\theta_0$, $\\theta_1$\n",
        " \n",
        " * Cost Function:\n",
        "    \n",
        "    $J(\\theta_0, \\theta_1)$ = $\\frac{1}{2m}$ $\\sum_{i=1}^{m}$ $(h_\\theta(X^{(i)}) - y^{(i)})^2$ \n",
        "    \n",
        " * Goal: \n",
        "   \n",
        "   $\\underset{\\theta_0, \\theta_1}{minimize}$ $J(\\theta_0, \\theta_1)$\n",
        "\n",
        "     ![cost_func](https://user-images.githubusercontent.com/15044221/52915185-f5e2b900-32fa-11e9-8b57-c86b272675f1.png)\n",
        "\n",
        "       Fig-2: Plot the Hypothesis Function where $\\theta_0$=$0$, $x$ is the size of house.\n",
        "       \n",
        "      ![cost_funct](https://user-images.githubusercontent.com/15044221/52915671-1f064800-3301-11e9-8502-6b622456ba7e.png)\n",
        "      \n",
        "      Fig-3: Plot the Cost Function where $\\theta_0$=$0$, $\\theta_1$=$1$.\n",
        "      \n",
        "       \n",
        " * **Simplified**\n",
        " \n",
        "  * Hypothesis: \n",
        " \n",
        "    $h_\\theta(x)$ = $\\theta_1x$\n",
        " \n",
        "  * Parameters:\n",
        " \n",
        "    $\\theta_1$\n",
        " \n",
        "  * Cost Function:\n",
        "    \n",
        "    $J(\\theta_1)$ = $\\frac{1}{2m}$ $\\sum_{i=1}^{m}$ $(h_\\theta(X^{(i)}) - y^{(i)})^2$ \n",
        "    \n",
        "  * Goal: \n",
        "   \n",
        "    $\\underset{\\theta_1}{minimize}$ $J(\\theta_1)$\n",
        "\n",
        "   *Two key functions we want to understand:*\n",
        "   \n",
        "   1. $h_\\theta(x)$ : For fixed $\\theta_1$, this is a function of $x$. So the hypothsis is a finction of, what is the size of the house $x$.\n",
        "   2. $J(\\theta_1)$: Function of the parameter $\\theta_1$. In contrast, $J$, that's a function of parameter, $\\theta_1$, which controls the slope of the straight line.\n",
        "    \n",
        "   \n",
        "   If we try to think of it in visual terms, our training data set is scattered on the x-y plane. We are trying to make a straight line (defined by $h_\\theta(x)$) which passes through these scattered data points.\n",
        "\n",
        "Our objective is to get the best possible line. The best possible line will be such so that the average squared vertical distances of the scattered points from the line will be the least. Ideally, the line should pass through all the points of our training data set. \n",
        "\n",
        "Thus as a goal, we should try to minimize the cost function. In this case, $\\theta_1$ = $1$ is our global minimum.\n",
        "   "
      ]
    },
    {
      "metadata": {
        "id": "NH1hJcJCc9WI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Parameter Learning\n",
        "-------------------------------------\n",
        "\n",
        "### Gradient Descent: \n",
        "\n",
        "   * Gradient descent is an algorithm for minimizing the cost function $J$. It turns out gradient descent is a more general algorithm, and is used not only in linear regression. It's actually used all over the place in machine learning.\n",
        "   \n",
        "   * Have some function, $J(\\theta_0, \\theta_1)$ \n",
        "   * Want,  $\\underset{\\theta_0, \\theta_1}{min}$ $J(\\theta_0, \\theta_1)$ \n",
        "   * Outline:\n",
        "     \n",
        "      * Start with some $\\theta_0$, $\\theta_1$\n",
        "      * Keep changing $\\theta_0$, $\\theta_1$ to reduce  $J(\\theta_0, \\theta_1)$  until we hopefully end up at a minimum.\n",
        "      \n",
        "   * **Gradient descent algorithm**\n",
        "   \n",
        "     * repeat until convergence {\n",
        "          \n",
        "          $\\theta_j$   := $\\theta_j$ - $\\alpha$$\\frac{\\delta}{\\delta\\theta_j}$ $J(\\theta_0, \\theta_1)$\n",
        "          (for $j$=$0$ and $j$=$1$)\n",
        "          \n",
        "        }\n",
        "        \n",
        "        Where, $\\alpha$ = $learning$ $rate$. \n",
        "        And what $\\alpha$ does is it basically controls how big a step we take downhill with creating descent. So if $\\alpha$ is very large, then that corresponds to a very aggressive gradient descent procedure where we're trying take huge steps downhill and if $\\alpha$ is very small, then we're taking little, little baby steps downhill.\n",
        "        \n",
        "       * If $\\alpha$ is too small, gradient descent can be slow.\n",
        "       * If $\\alpha$ is too large, gradient descent can overshoot the minimum. It may fail to converge, or even diverge.\n",
        "\n",
        "     * What if your parameter $\\theta_1$ is already at a local minimum, what do you think one step of gradient descent will do?\n",
        "      \n",
        "       * **Ans:** Slope will be equal to $0$. So, Leave $\\theta_1$ unchanged. So if your parameters are already at a local minimum one step with gradient descent does not absolutely nothing it doesn't your parameter, which is what you want beacause it keeps your solution at the local optimum.\n",
        "       \n",
        "     * Why gradient descent can converse the local minimum even with the learning rate alpha fixed?\n",
        "      \n",
        "       * **Ans:** Gradient descent can converge to a local minimum, even with the learning rate $\\alpha$ fixed.\n",
        "      \n",
        "          $\\theta_1$   := $\\theta_1$ - $\\alpha$$\\frac{\\delta}{\\delta\\theta_1}$ $J(\\theta_1)$\n",
        "          \n",
        "          As we approach a local minimum, gradient descent will automatically take smaller steps. So, on need to decrese $\\alpha$ over time.\n",
        "          \n",
        "          So that's the gradient descent algorithm and you can use it to try to minimize any cost function $J$, not the cost function $J$ that we defiend for linear regression.\n",
        "          \n",
        "     \n"
      ]
    },
    {
      "metadata": {
        "id": "qs-vOXIfY_cR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Gradient descent for linear regression\n",
        "-------------------------------------------------------------------\n",
        "\n",
        "### Linear Regression Model:\n",
        "\n",
        "* **Hypothesis Function:**  $h_\\theta(x) = \\theta_0 + \\theta_1x$\n",
        "\n",
        "* ** Squared Error Cost Function: ** $J(\\theta_0, \\theta_1)$ = $\\frac{1}{2m}$ $\\sum_{i=1}^{m}$ $(h_\\theta(x^{(i)}) - y^{(i)})^2$\n",
        "\n",
        "     Apply gradient descent to minimize our squared error cost function.\n",
        "     \n",
        "     The key term we need is cost function $J$ derivative term over here. So, we need to figure out what is this partial deivative term.\n",
        "     \n",
        "     $\\frac{\\delta}{\\delta\\theta_j} J(\\theta_0, \\theta_1) = \\frac{\\delta}{\\delta\\theta_j} \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2$\n",
        "     \n",
        "     $\\frac{\\delta}{\\delta\\theta_j} J(\\theta_0, \\theta_1) = \\frac{\\delta}{\\delta\\theta_j} \\frac{1}{2m} \\sum_{i=1}^{m} ((\\theta_0 + \\theta_1x^{(i)}) - y^{(i)})^2$\n",
        "     \n",
        "  Partial derivative with respect to $\\theta_0$ and $\\theta_1$.\n",
        "     \n",
        "    *  $\\theta_0, J=0: \\frac{\\delta}{\\delta\\theta_0} J(\\theta_0, \\theta_1) =  \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})$\n",
        "    \n",
        "    *  $\\theta_1, J=1: \\frac{\\delta}{\\delta\\theta_1} J(\\theta_0, \\theta_1) =  \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}).x^{(i)}$\n",
        "    \n",
        " \n",
        "* ***Gradient descent algorithm for linear regression:***\n",
        "\n",
        "    * repeat until convergence {\n",
        "    \n",
        "    $\\theta_0 := \\theta_0 - \\alpha \\frac{\\delta}{\\delta\\theta_j} \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})$\n",
        "    \n",
        "    $\\theta_1 := \\theta_1 - \\alpha \\frac{\\delta}{\\delta\\theta_j} \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}).x^{(i)}$\n",
        "    \n",
        "    }\n",
        "    \n",
        " \n",
        " * ** \"Batch\" Gradient Descent:**\n",
        "     \n",
        "     \"Batch\" : Each step of gradient descent uses all the training examples.\n",
        "     \n",
        "     \n"
      ]
    },
    {
      "metadata": {
        "id": "uoQqlQFO7zpd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Linear Algebra Review\n",
        "------------------------------------------\n",
        "\n",
        "* **Matrices and Vectors**\n",
        "   \n",
        "   * **Matrix:** Rectangular array of Numbers.\n",
        "   * **Dimension of Matrix:** Number of rows $x$ numbers of columns.\n",
        "   * **Vector:** An $nx1$ matrix. Also called $n$-D vector. Also denoted by $\\mathbb{R^n}$.\n",
        "   \n",
        "   * **Notation and terms:**\n",
        "\n",
        "      * $A_{ij}$refers to the element in the $i$-th row and $j$-th column of matrix $A$.\n",
        "\n",
        "      * $A$ vector with $'n'$ rows is referred to as an $'n'$-dimensional vector.\n",
        "\n",
        "     *  $v_i$  refers to the element in the $i$-th row of the vector.\n",
        "     \n",
        "     * In general, all our vectors and matrices will be $1$-indexed. Note that for some programming languages, the arrays are $0$-indexed.\n",
        "     \n",
        "     * Matrices are usually denoted by uppercase names while vectors are lowercase.\n",
        "\n",
        "     * \"Scalar\" means that an object is a single value, not a vector or matrix.\n",
        "\n",
        "     * $\\mathbb{R}$ refers to the set of scalar real numbers.\n",
        "\n",
        "     * $\\mathbb{R^n}$ refers to the set of $n$-dimensional vectors of real numbers.\n",
        "\n",
        "   \n",
        "* **Addition and scalar multiplication**\n",
        "\n",
        "  * Addition and subtraction are element-wise, so you simply add or subtract each corresponding element:\n",
        "  \n",
        "      $\\begin{bmatrix} a & b \\newline c & d \\newline \\end{bmatrix} + \\begin{bmatrix} w & x \\newline y & z \\newline \\end{bmatrix} =\\begin{bmatrix} a+w & b+x \\newline c+y & d+z \\newline \\end{bmatrix}$\n",
        "      \n",
        "  * In scalar division, we simply divide every element by the scalar value:\n",
        "\n",
        "      $\\begin{bmatrix} a & b \\newline c & d \\newline \\end{bmatrix} / x =\\begin{bmatrix} a /x & b/x \\newline c /x & d /x \\newline \\end{bmatrix}$\n",
        "      \n",
        "\n",
        "* ** Matrix-Vector multiplication **\n",
        "\n",
        "  * $Prediction = DataMetrix  * Parameters$\n",
        "  \n",
        "  * We map the column of the vector onto each row of the matrix, multiplying each element and summing the result.\n",
        "     \n",
        "     $\\begin{bmatrix} a & b \\newline c & d \\newline e & f \\end{bmatrix} *\\begin{bmatrix} x \\newline y \\newline \\end{bmatrix} =\\begin{bmatrix} a*x + b*y \\newline c*x + d*y \\newline e*x + f*y\\end{bmatrix}$\n",
        "     \n",
        "     The result is a vector. The number of columns of the matrix must equal the number of rows of the vector.\n",
        "\n",
        "    An m x n matrix multiplied by an n x 1 vector results in an m x 1 vector.\n",
        "    \n",
        "    \n",
        " * **Matrix-Matrix Multiplication**\n",
        "    \n",
        "    * We multiply two matrices by breaking it into several vector multiplications and concatenating the result\n",
        "    \n",
        "    $\\begin{bmatrix} a & b \\newline c & d \\newline e & f \\end{bmatrix} *\\begin{bmatrix} w & x \\newline y & z \\newline \\end{bmatrix} =\\begin{bmatrix} a*w + b*y & a*x + b*z \\newline c*w + d*y & c*x + d*z \\newline e*w + f*y & e*x + f*z\\end{bmatrix}$\n",
        "    \n",
        "    An m x n matrix multiplied by an n x o matrix results in an m x o matrix. In the above example, a 3 x 2 matrix times a 2 x 2 matrix resulted in a 3 x 2 matrix.\n",
        "\n",
        "    To multiply two matrices, the number of columns of the first matrix must equal the number of rows of the second matrix.\n",
        "    \n",
        "    \n",
        " * ** Matrix Multiplication Properties**\n",
        " \n",
        "    * Matrices are not commutative: $A∗B \\neq B∗A$\n",
        "    * Matrices are associative: $(A∗B)∗C =  A∗(B∗C)$\n",
        "    * The identity matrix, when multiplied by any matrix of the same dimensions, results in the original matrix. It's just like multiplying numbers by 1. The identity matrix simply has 1's on the diagonal (upper left to lower right diagonal) and 0's elsewhere.\n",
        "    \n",
        "    $\\begin{bmatrix} 1 & 0 & 0 \\newline 0 & 1 & 0 \\newline 0 & 0 & 1 \\newline \\end{bmatrix}$\n",
        "\n",
        "   * When multiplying the identity matrix after some matrix (A∗I), the square identity matrix's dimension should match the other matrix's columns. When multiplying the identity matrix before some other matrix (I∗A), the square identity matrix's dimension should match the other matrix's rows. \n",
        "   \n",
        "   \n",
        " * **Inverse and Transpose**\n",
        " \n",
        "   * Matrices that don't have inverse are \"singular\" or \"degenerate\".\n",
        "   * Let $A$ be an $(mxn)$ matrix, and let $B = A^T$. Then $B$ is an $(nxm)$ matrix, and $B_{ij} = A_{ji}$\n",
        "   * The inverse of a matrix $A$ is denoted $A^{-1}$. Multiplying by the inverse results in the identity matrix.\n",
        "   * A non square matrix does not have an inverse matrix. We can compute inverses of matrices in octave with the $pinv(A)$ function and in Matlab with the $inv(A)$  function. Matrices that don't have an inverse are *singular* or *degenerate*.\n",
        "   * The transposition of a matrix is like rotating the matrix $90°$ in clockwise direction and then reversing it. We can compute transposition of matrices in matlab with the $transpose(A)$ function or $A'$:\n",
        "   \n",
        "       $A = \\begin{bmatrix} a & b \\newline c & d \\newline e & f \\end{bmatrix}$\n",
        "   \n",
        "       $A^T = \\begin{bmatrix} a & c & e \\newline b & d & f \\newline \\end{bmatrix}$\n",
        "       \n",
        "   * In other words: $A_{ij} = A^{T}_{ji}$\n",
        "   "
      ]
    },
    {
      "metadata": {
        "id": "O3OJlVBkdD3J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Linear Algebra Review: Python Code "
      ]
    },
    {
      "metadata": {
        "id": "gm-xxXO81XiB",
        "colab_type": "code",
        "outputId": "d86596e6-c79a-412d-8165-a581d465cf22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "cell_type": "code",
      "source": [
        "## Matrix and Vector\n",
        "##-------------------\n",
        "\n",
        "### Initialize matrix A with 4 rows and 3 cols\n",
        "# Load library\n",
        "import numpy as np\n",
        "A = np.matrix([[1,2,3], [4,5,6], [7,8, 9],[10,11,12]])\n",
        "print(\"Matrix: \\n\", A)\n",
        "\n",
        "### Initialize a vector\n",
        "v = np.matrix([1,2,3])\n",
        "print(\"Vector: \", v)\n",
        "\n",
        "\n",
        "### Get the dimension of the matrix A where m = rows and n = cols\n",
        "[m, n] = A.shape\n",
        "print(\"Dim A: \", [m, n])\n",
        "\n",
        "###You could also store it this way\n",
        "dim_A = A.shape\n",
        "print(\"dim_A: \", dim_A)\n",
        "\n",
        "### Get the dimension of the vector v\n",
        "dim_v = v.shape\n",
        "print(\"dim_v: \", dim_v)\n",
        "\n",
        "### Now let's index into the 2nd row and 3rth cols of matrix A.\n",
        "A_23 = A.item(2-1, 3-1)\n",
        "print(\"A_23: \", A_23)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matrix: \n",
            " [[ 1  2  3]\n",
            " [ 4  5  6]\n",
            " [ 7  8  9]\n",
            " [10 11 12]]\n",
            "Vector:  [[1 2 3]]\n",
            "Dim A:  [4, 3]\n",
            "dim_A:  (4, 3)\n",
            "dim_v:  (1, 3)\n",
            "A_23:  6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WWbUCLfM1YW8",
        "colab_type": "code",
        "outputId": "cdd94dac-1a28-4b0f-dab1-cc7288859c2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "cell_type": "code",
      "source": [
        "## %%%%%%%%% Addition and Scalar Multiplication %%%%%%%%%%%%\n",
        "## %--------------------------------------------------\n",
        "\n",
        "## % Initialize matrix A and B\n",
        "A = np.matrix([[1,2,4], [5,3,2]])\n",
        "B = np.matrix([[1,3,4], [1,1,1]])\n",
        "print(\"Matrix A: \\n\", A)\n",
        "print(\"Matrix B: \\n\", B)\n",
        "## % Initialize constant s\n",
        "s = 2\n",
        "\n",
        "## % See how the element-wise addition works.\n",
        "add_AB = A + B\n",
        "print(\"add_AB: \\n\", add_AB)\n",
        "\n",
        "## % See how the element-wise subtraction works.\n",
        "sub_AB = A - B\n",
        "print(\"sub_AB: \\n\", sub_AB)\n",
        "\n",
        "## % Divide A by s\n",
        "div_As = A / s\n",
        "print(\"div_As: \\n\", div_As)\n",
        "\n",
        "## % What happens if we have a Matrix  + scalar?\n",
        "add_As = A + s\n",
        "print(\"add_As: \\n\", add_As)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matrix A: \n",
            " [[1 2 4]\n",
            " [5 3 2]]\n",
            "Matrix B: \n",
            " [[1 3 4]\n",
            " [1 1 1]]\n",
            "add_AB: \n",
            " [[2 5 8]\n",
            " [6 4 3]]\n",
            "sub_AB: \n",
            " [[ 0 -1  0]\n",
            " [ 4  2  1]]\n",
            "div_As: \n",
            " [[0.5 1.  2. ]\n",
            " [2.5 1.5 1. ]]\n",
            "add_As: \n",
            " [[3 4 6]\n",
            " [7 5 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e9PyMYB33U50",
        "colab_type": "code",
        "outputId": "8cd27b12-2e4a-4bcc-e591-8f6e80f345b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "cell_type": "code",
      "source": [
        "### %%%%% Matrix Vector Multiplication\n",
        "## %%%-------------------------------\n",
        "### % Initialize a 3 by 2 matrix \n",
        "A = np.matrix([[1, 2], [3, 4],[5, 6]])\n",
        "print(\"Matrix A: \\n\", A)\n",
        "\n",
        "### % Initialize a 2 by 1 matrix \n",
        "B = np.matrix([1, 2])\n",
        "print(\"Matrix: \\n\", B)\n",
        "\n",
        "### % Multiply A * B\n",
        "mult_AB = A.dot(B.T)\n",
        "print(\"mult_AB: \\n\", mult_AB)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matrix A: \n",
            " [[1 2]\n",
            " [3 4]\n",
            " [5 6]]\n",
            "Matrix: \n",
            " [[1 2]]\n",
            "mult_AB: \n",
            " [[ 5]\n",
            " [11]\n",
            " [17]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EhG40oPv4CE9",
        "colab_type": "code",
        "outputId": "cc556f71-3140-46a9-8912-dedd3fa3cc56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "cell_type": "code",
      "source": [
        "### %%%% Matrix Multiplication Properties\n",
        "## %%------------------------------------\n",
        "## % Initialize random matrices A and B \n",
        "A = np.matrix([[1,2],[4,5]])\n",
        "B = np.matrix([[1,1],[0,2]])\n",
        "print(\"Matrix A: \\n\", A)\n",
        "print(\"Matrix B: \\n\", B)\n",
        "\n",
        "## % Initialize a 2 by 2 identity matrix\n",
        "I = np.identity(2)\n",
        "print(\"Identity Matrix: \\n\", I)\n",
        "\n",
        "### % The above notation is the same as I = [1,0;0,1]\n",
        "\n",
        "## % What happens when we multiply I*A ? \n",
        "IA = I.dot(A)\n",
        "print(\"I*A: \\n\", IA)\n",
        "\n",
        "## % How about A*I ? \n",
        "AI = A.dot(I)\n",
        "print(\"A*I: \\n\", AI)\n",
        "\n",
        "## % Compute A*B \n",
        "AB = np.dot(A, B)\n",
        "print(\"A*B: \\n\", AB)\n",
        "\n",
        "## % Is it equal to B*A? \n",
        "BA = np.dot(B, A)\n",
        "print(\"B*A: \\n\", BA)\n",
        "\n",
        "### % Note that IA = AI but AB != BA\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matrix A: \n",
            " [[1 2]\n",
            " [4 5]]\n",
            "Matrix B: \n",
            " [[1 1]\n",
            " [0 2]]\n",
            "Identity Matrix: \n",
            " [[1. 0.]\n",
            " [0. 1.]]\n",
            "I*A: \n",
            " [[1. 2.]\n",
            " [4. 5.]]\n",
            "A*I: \n",
            " [[1. 2.]\n",
            " [4. 5.]]\n",
            "A*B: \n",
            " [[ 1  5]\n",
            " [ 4 14]]\n",
            "B*A: \n",
            " [[ 5  7]\n",
            " [ 8 10]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j9sX1FkS56Zj",
        "colab_type": "code",
        "outputId": "bb9fb094-65e0-406d-b542-55bceda4cffd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "cell_type": "code",
      "source": [
        "### %%%%% Inverse and Transpose\n",
        "### %%-------------------------\n",
        "\n",
        "## % Initialize matrix A\n",
        "A = np.matrix([[1,2,0], [0,5,6], [7,0,9]])\n",
        "print(\"Matrix A: \\n\", A)\n",
        "\n",
        "## % Transpose A\n",
        "A_trans = A.T\n",
        "print(\"Matrix A_trans: \\n\", A_trans)\n",
        "\n",
        "## % Take the inverse of A\n",
        "A_inv = A.I\n",
        "print(\"A_inv: \\n\", A_inv)\n",
        "\n",
        "## % What is A^(-1)*A?\n",
        "A_invA = np.dot(A_inv, A)\n",
        "print(\"A_invA: \\n\", A_invA)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matrix A: \n",
            " [[1 2 0]\n",
            " [0 5 6]\n",
            " [7 0 9]]\n",
            "Matrix A_trans: \n",
            " [[1 0 7]\n",
            " [2 5 0]\n",
            " [0 6 9]]\n",
            "A_inv: \n",
            " [[ 0.34883721 -0.13953488  0.09302326]\n",
            " [ 0.3255814   0.06976744 -0.04651163]\n",
            " [-0.27131783  0.10852713  0.03875969]]\n",
            "A_invA: \n",
            " [[ 1.00000000e+00 -8.32667268e-17  5.55111512e-17]\n",
            " [ 2.77555756e-17  1.00000000e+00 -8.32667268e-17]\n",
            " [-3.46944695e-17  2.77555756e-17  1.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UU7hLQSblhNv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***References***\n",
        "\n",
        "  [1]. [Machine Learning by Andrew NG.](https://www.coursera.org/learn/machine-learning/home/welcome)"
      ]
    }
  ]
}